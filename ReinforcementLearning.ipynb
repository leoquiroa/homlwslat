{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy gradients and deep Q-networks (DQNs)\n",
    "* Markov decision processes (MDPs)\n",
    "* The policy can be any algorithm you can think of, and it does not have to be deterministic. In fact, in some cases it does not even have to observe the environment!\n",
    "* It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.\n",
    "* If the policy involves some randomness, it is called a stochastic policy\n",
    "* Two policy parameters you can tweak: the probability p and the angle range r. This is an example of policy search, in this case using a brute force approach\n",
    "* When the policy space is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack.\n",
    "* Genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent7 plus some \n",
    "random variation. The surviving policies plus their offspring together constitute the second generation. You can continue to iterate through generations this way until you find a good policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "import PIL.Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole-v1\n",
    "\n",
    "* After the environment is created, you must initialize it using the reset() method. This returns the first observation.\n",
    "* Each observation is a 1D NumPy array containing four floats: these floats represent the cart’s horizontal position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 = vertical), and its angular velocity (positive means clockwise).\n",
    "* To display this environment by calling its render() method what actions are possible\n",
    "* The step() method executes the given action and returns four values\n",
    "    * obs > This is the new observation.\n",
    "    * reward > In this environment, you get a reward of 1.0 at every step, no matter what you do\n",
    "    * done > This value will be True when the episode is over.\n",
    "    * info > This environment-specific dictionary can provide some extra information that you may find useful for debugging or for training\n",
    "* Once finished using an environment, you should call its close() method to free resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02182142 -0.01466921 -0.00882932  0.04596068]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That means that the possible actions are integers 0 and 1, which represent accelerating left (0) or right (1).\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = io.BytesIO()\n",
    "    ima = PIL.Image.fromarray(a).save(f, fmt)\n",
    "    return f.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image(env):\n",
    "    mode_rgb = env.render(mode='rgb_array')\n",
    "    show = showarray(mode_rgb)\n",
    "    return display.Image(data=show, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 450
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "imagehandle = display.display(image(env), display_id='gymscr')\n",
    "\n",
    "for _ in range(100):\n",
    "    time.sleep(0.01)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action) # take a random action\n",
    "    display.update_display(image(env), display_id='gymscr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example1\n",
    "\n",
    "Observation values\n",
    "1. the cart’s horizontal position (0.0 = center), \n",
    "2. its velocity (positive means right), \n",
    "3. the angle of the pole (0.0 = vertical), \n",
    "4. its angular velocity (positive means clockwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2] # get the angle\n",
    "    return 0 if angle < 0 else 1 \n",
    "    # if < 0 then 0\n",
    "    # if >= 0 then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 450
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.00881079 -0.20501064 -0.02837621  0.27633896] 1.0 False {}\n",
      "0 [ 0.00471058 -0.3997165  -0.02284943  0.55993869] 1.0 False {}\n",
      "0 [-0.00328375 -0.59451041 -0.01165066  0.84533614] 1.0 False {}\n",
      "0 [-0.01517396 -0.78947148  0.00525606  1.13433268] 1.0 False {}\n",
      "1 [-0.03096339 -0.5944187   0.02794272  0.84330283] 1.0 False {}\n",
      "1 [-0.04285176 -0.39968902  0.04480877  0.55953642] 1.0 False {}\n",
      "1 [-0.05084554 -0.2052237   0.0559995   0.2813006 ] 1.0 False {}\n",
      "1 [-0.05495002 -0.0109434   0.06162551  0.00679222] 1.0 False {}\n",
      "1 [-0.05516889  0.1832431   0.06176136 -0.26582817] 1.0 False {}\n",
      "1 [-0.05150402  0.37743168  0.05644479 -0.53840937] 1.0 False {}\n",
      "1 [-0.04395539  0.57171656  0.04567661 -0.81278661] 1.0 False {}\n",
      "1 [-0.03252106  0.76618411  0.02942087 -1.09075944] 1.0 False {}\n",
      "1 [-0.01719738  0.96090618  0.00760569 -1.37406766] 1.0 False {}\n",
      "1 [ 0.00202075  1.15593225 -0.01987567 -1.66436224] 1.0 False {}\n",
      "0 [ 0.02513939  0.96104719 -0.05316291 -1.37793546] 1.0 False {}\n",
      "0 [ 0.04436034  0.76662797 -0.08072162 -1.10234118] 1.0 False {}\n",
      "0 [ 0.0596929   0.57265535 -0.10276844 -0.83603607] 1.0 False {}\n",
      "0 [ 0.071146    0.37907601 -0.11948917 -0.57735982] 1.0 False {}\n",
      "0 [ 0.07872752  0.18581362 -0.13103636 -0.32457956] 1.0 False {}\n",
      "0 [ 0.08244379 -0.00722281 -0.13752795 -0.07592181] 1.0 False {}\n",
      "0 [ 0.08229934 -0.20013265 -0.13904639  0.17040488] 1.0 False {}\n",
      "0 [ 0.07829669 -0.39301889 -0.13563829  0.41619377] 1.0 False {}\n",
      "0 [ 0.07043631 -0.58598404 -0.12731442  0.66322748] 1.0 False {}\n",
      "0 [ 0.05871663 -0.77912643 -0.11404987  0.91326686] 1.0 False {}\n",
      "0 [ 0.0431341  -0.97253621 -0.09578453  1.16803874] 1.0 False {}\n",
      "0 [ 0.02368337 -1.16629048 -0.07242376  1.42922033] 1.0 False {}\n",
      "0 [ 3.57564606e-04 -1.36044721e+00 -4.38393487e-02  1.69841800e+00] 1.0 False {}\n",
      "0 [-0.02685138 -1.55503724 -0.00987099  1.97713796] 1.0 False {}\n",
      "0 [-0.05795212 -1.75005389  0.02967177  2.26674666] 1.0 False {}\n",
      "1 [-0.0929532  -1.55522105  0.0750067   1.98334827] 1.0 False {}\n",
      "1 [-0.12405762 -1.36096301  0.11467367  1.71481179] 1.0 False {}\n",
      "1 [-0.15127688 -1.16732851  0.1489699   1.45990789] 1.0 False {}\n",
      "1 [-0.17462345 -0.97431403  0.17816806  1.2172281 ] 1.0 False {}\n",
      "1 [-0.19410973 -0.78187961  0.20251262  0.98525055] 1.0 False {}\n",
      "1 [-0.20974733 -0.58996084  0.22221764  0.76238797] 1.0 True {}\n",
      "[-0.20974733 -0.58996084  0.22221764  0.76238797] 1.0 True {}\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = 0\n",
    "totals = []\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "imagehandle = display.display(image(env), display_id='gymscr')\n",
    "for _ in range(200):\n",
    "    time.sleep(0.01)\n",
    "    action = basic_policy(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_rewards += reward\n",
    "    print(action,obs,reward,done,info)\n",
    "    if done: \n",
    "        print(obs,reward,done,info)\n",
    "        break\n",
    "    display.update_display(image(env), display_id='gymscr')\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.5, 14.430869689661812, 1.0, 50.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finish after  np.max(totals) observations\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Policies\n",
    "* why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score. This approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well.\n",
    "* In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment’s full state\n",
    "*  If there were some hidden state, then you might need to consider past actions and observations as well.\n",
    "* For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the total input of neurons is 4, env.observation_space.shape[0]\n",
    "* the otput_neuront is the probability of going left p. Then, p-1 is for going right\n",
    "* the activation function is sigmoid because it is only one output action\n",
    "* If there were more than two possible actions, there would be one output neuron per action, and we would use the softmax activation function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 5\n",
    "output_neuron = 1 \n",
    "n_inputs = 4 \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(hidden_units, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(output_neuron, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability distribution and the target probability distribution. It would just be regular supervised learning\n",
    "* In Reinforcement Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed\n",
    "*  For example, if the agentmanages to balance the pole for 100 steps, how can it know which of the 100 actions it took were good, and which of them were bad?\n",
    "* This is called the credit assignment problem: when the agent gets a reward, it is hard for it to know which actions should get credited (or blamed) for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Credit Assignment Problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
