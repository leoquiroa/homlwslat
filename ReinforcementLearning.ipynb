{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy gradients and deep Q-networks (DQNs)\n",
    "* Markov decision processes (MDPs)\n",
    "* The policy can be any algorithm you can think of, and it does not have to be deterministic. In fact, in some cases it does not even have to observe the environment!\n",
    "* It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.\n",
    "* If the policy involves some randomness, it is called a stochastic policy\n",
    "* Two policy parameters you can tweak: the probability p and the angle range r. This is an example of policy search, in this case using a brute force approach\n",
    "* When the policy space is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack.\n",
    "* Genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent7 plus some \n",
    "random variation. The surviving policies plus their offspring together constitute the second generation. You can continue to iterate through generations this way until you find a good policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "import PIL.Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole-v1\n",
    "\n",
    "* After the environment is created, you must initialize it using the reset() method. This returns the first observation.\n",
    "* Each observation is a 1D NumPy array containing four floats: these floats represent the cart’s horizontal position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 = vertical), and its angular velocity (positive means clockwise).\n",
    "* To display this environment by calling its render() method what actions are possible\n",
    "* The step() method executes the given action and returns four values\n",
    "    * obs > This is the new observation.\n",
    "    * reward > In this environment, you get a reward of 1.0 at every step, no matter what you do\n",
    "    * done > This value will be True when the episode is over.\n",
    "    * info > This environment-specific dictionary can provide some extra information that you may find useful for debugging or for training\n",
    "* Once finished using an environment, you should call its close() method to free resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01150457  0.0398663  -0.03032701  0.03706546]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That means that the possible actions are integers 0 and 1, which represent accelerating left (0) or right (1).\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = io.BytesIO()\n",
    "    ima = PIL.Image.fromarray(a).save(f, fmt)\n",
    "    return f.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image(env):\n",
    "    mode_rgb = env.render(mode='rgb_array')\n",
    "    show = showarray(mode_rgb)\n",
    "    return display.Image(data=show, width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 450
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.8.6/envs/vml/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "imagehandle = display.display(image(env), display_id='gymscr')\n",
    "\n",
    "for _ in range(100):\n",
    "    time.sleep(0.01)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action) # take a random action\n",
    "    display.update_display(image(env), display_id='gymscr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example1\n",
    "\n",
    "Observation values\n",
    "1. the cart’s horizontal position (0.0 = center), \n",
    "2. its velocity (positive means right), \n",
    "3. the angle of the pole (0.0 = vertical), \n",
    "4. its angular velocity (positive means clockwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2] # get the angle\n",
    "    return 0 if angle < 0 else 1 \n",
    "    # if < 0 then 0\n",
    "    # if >= 0 then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "width": 450
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-0.04253222  0.17163967  0.04464734 -0.326816  ] 1.0 False {}\n",
      "1 [-0.03909943  0.36609847  0.03811102 -0.60509157] 1.0 False {}\n",
      "1 [-0.03177746  0.56066732  0.02600919 -0.885531  ] 1.0 False {}\n",
      "1 [-0.02056411  0.75542669  0.00829857 -1.1699254 ] 1.0 False {}\n",
      "1 [-0.00545558  0.95043973 -0.01509994 -1.45999514] 1.0 False {}\n",
      "0 [ 0.01355321  0.75550615 -0.04429985 -1.17206733] 1.0 False {}\n",
      "0 [ 0.02866334  0.56098726 -0.06774119 -0.89359513] 1.0 False {}\n",
      "0 [ 0.03988308  0.3668462  -0.08561309 -0.62295214] 1.0 False {}\n",
      "0 [ 0.04722001  0.1730174  -0.09807214 -0.35841332] 1.0 False {}\n",
      "0 [ 0.05068035 -0.02058351 -0.1052404  -0.0981944 ] 1.0 False {}\n",
      "0 [ 0.05026868 -0.21405206 -0.10720429  0.15951923] 1.0 False {}\n",
      "0 [ 0.04598764 -0.40748891 -0.10401391  0.41655103] 1.0 False {}\n",
      "0 [ 0.03783786 -0.60099483 -0.09568289  0.67471621] 1.0 False {}\n",
      "0 [ 0.02581797 -0.79466599 -0.08218856  0.93580628] 1.0 False {}\n",
      "0 [ 0.00992465 -0.98858905 -0.06347244  1.20157272] 1.0 False {}\n",
      "0 [-0.00984713 -1.18283532 -0.03944098  1.47370702] 1.0 False {}\n",
      "0 [-0.03350384 -1.37745362 -0.00996684  1.7538148 ] 1.0 False {}\n",
      "0 [-0.06105291 -1.57246113  0.02510945  2.04338134] 1.0 False {}\n",
      "1 [-0.09250213 -1.37760585  0.06597708  1.75857195] 1.0 False {}\n",
      "1 [-0.12005425 -1.18329025  0.10114852  1.4871159 ] 1.0 False {}\n",
      "1 [-0.14372006 -0.98953573  0.13089084  1.22765656] 1.0 False {}\n",
      "1 [-0.16351077 -0.7963185   0.15544397  0.97868201] 1.0 False {}\n",
      "1 [-0.17943714 -0.60358346  0.17501761  0.73858189] 1.0 False {}\n",
      "1 [-0.19150881 -0.41125459  0.18978925  0.50568865] 1.0 False {}\n",
      "1 [-0.1997339  -0.21924255  0.19990302  0.27830592] 1.0 False {}\n",
      "1 [-0.20411875 -0.0274501   0.20546914  0.05472715] 1.0 False {}\n",
      "1 [-0.20466775  0.16422414  0.20656368 -0.16675274] 1.0 False {}\n",
      "1 [-0.20138327  0.35588287  0.20322863 -0.38783049] 1.0 False {}\n",
      "1 [-0.19426561  0.5476279   0.19547202 -0.61019011] 1.0 False {}\n",
      "1 [-0.18331306  0.73955812  0.18326821 -0.83549929] 1.0 False {}\n",
      "1 [-0.16852189  0.9317672   0.16655823 -1.06540487] 1.0 False {}\n",
      "1 [-0.14988655  1.12434069  0.14525013 -1.30152561] 1.0 False {}\n",
      "1 [-0.12739974  1.31735206  0.11921962 -1.54544043] 1.0 False {}\n",
      "1 [-0.1010527   1.51085722  0.08831081 -1.79867026] 1.0 False {}\n",
      "1 [-0.07083555  1.70488718  0.05233741 -2.06265139] 1.0 False {}\n",
      "1 [-0.03673781  1.89943809  0.01108438 -2.33869799] 1.0 False {}\n",
      "1 [ 1.25095479e-03  2.09445848e+00 -3.56895826e-02 -2.62795187e+00] 1.0 False {}\n",
      "0 [ 0.04314012  1.89962567 -0.08824862 -2.34637927] 1.0 False {}\n",
      "0 [ 0.08113264  1.70539978 -0.13517621 -2.08208557] 1.0 False {}\n",
      "0 [ 0.11524063  1.51188012 -0.17681792 -1.83407502] 1.0 False {}\n",
      "0 [ 0.14547824  1.31910065 -0.21349942 -1.60112843] 1.0 True {}\n",
      "[ 0.14547824  1.31910065 -0.21349942 -1.60112843] 1.0 True {}\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = 0\n",
    "totals = []\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "imagehandle = display.display(image(env), display_id='gymscr')\n",
    "for _ in range(200):\n",
    "    time.sleep(0.01)\n",
    "    action = basic_policy(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_rewards += reward\n",
    "    print(action,obs,reward,done,info)\n",
    "    if done: \n",
    "        print(obs,reward,done,info)\n",
    "        break\n",
    "    display.update_display(image(env), display_id='gymscr')\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.5, 11.543396380615196, 1.0, 40.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finish after  np.max(totals) observations\n",
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Policies\n",
    "* why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score. This approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well.\n",
    "* In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment’s full state\n",
    "*  If there were some hidden state, then you might need to consider past actions and observations as well.\n",
    "* For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the total input of neurons is 4, env.observation_space.shape[0]\n",
    "* the otput_neuront is the probability of going left p. Then, p-1 is for going right\n",
    "* the activation function is sigmoid because it is only one output action\n",
    "* If there were more than two possible actions, there would be one output neuron per action, and we would use the softmax activation function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 5\n",
    "output_neuron = 1 \n",
    "n_inputs = 4 \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(hidden_units, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(output_neuron, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Credit Assignment Problem\n",
    "* If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability distribution and the target probability distribution. It would just be regular supervised learning\n",
    "* In Reinforcement Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed\n",
    "*  For example, if the agent manages to balance the pole for 100 steps, how can it know which of the 100 actions it took were good, and which of them were bad?\n",
    "* This is called the credit assignment problem: when the agent gets a reward, it is hard for it to know which actions should get credited (or blamed) for it.\n",
    "* a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a discount factor γ (gamma) at each step\n",
    "* If the discount factor is close to 0, then future rewards won’t count for much compared to immediate rewards. \n",
    "* Conversely, if the discount factor is close to 1, then rewards far into the future will count almost as much as immediate rewards.\n",
    "* Typical discount factors vary from 0.9 to 0.99.\n",
    "* With a discount factor of 0.95, rewards 13 steps into the future count roughly for half as much as immediate rewards (since 0.9513 ≈ 0.5)\n",
    "* The action advantage, estimate how much better or worse an action is compared to the other possible actions. For this, we must run many episodes and normalize all the action returns (by subtracting the mean and dividing by the standard deviation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PG algorithms directly try to optimize the policy to increase rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play multiple episodes, returning all the rewards and gradients for each episode and each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "...                                discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let’s define the hyperparameters\n",
    "n_iterations = 10\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    print(iteration)\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f53c0066a30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "* the agent learns to estimate the expected return for each state, or for each action in each state, then it uses this knowledge to decide how to act\n",
    "* The Stochastic processes with no memory are called Markov chains. \n",
    "* Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. \n",
    "* The probability for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s′), not on past states (this is why we say that the system has no memory).\n",
    "* Markov decision processes resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.91891892 17.02702702 13.62162162]\n",
      " [ 0.                -inf -4.87971488]\n",
      " [       -inf 50.13365013        -inf]]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions\n",
    "gamma = 0.90 # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "print(Q_values)\n",
    "print(np.argmax(Q_values, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
